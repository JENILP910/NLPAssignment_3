{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGDiH4v2Cr3Pnt3FJ/rPRL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6037de7ef61e4406b19cf3bc2cdb9ee2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5070e42058cd47b2b0f9443b30b2a27c","IPY_MODEL_5d230aae987f45bfa04b40834629d7f4","IPY_MODEL_7d57b36a2ed246059cea60f278525a0c"],"layout":"IPY_MODEL_7c0536b2cb3849e79d35a375a8d661d9"}},"5070e42058cd47b2b0f9443b30b2a27c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1cdaa9a7aecc457b900e44fc387fa848","placeholder":"​","style":"IPY_MODEL_0d9fa876669e4ffe88bbeb9cd6d80ada","value":"README.md: 100%"}},"5d230aae987f45bfa04b40834629d7f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_45b79e8e3e854abea9e7efa4918ba9c8","max":8916,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e851b8ae16e54adab37dabe7d2d25dd4","value":8916}},"7d57b36a2ed246059cea60f278525a0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ce61eadf1094adcafecaa4afb0a78f8","placeholder":"​","style":"IPY_MODEL_eed1d436671b4c13abb0ef7e268d5b6b","value":" 8.92k/8.92k [00:00&lt;00:00, 399kB/s]"}},"7c0536b2cb3849e79d35a375a8d661d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cdaa9a7aecc457b900e44fc387fa848":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d9fa876669e4ffe88bbeb9cd6d80ada":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45b79e8e3e854abea9e7efa4918ba9c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e851b8ae16e54adab37dabe7d2d25dd4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ce61eadf1094adcafecaa4afb0a78f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eed1d436671b4c13abb0ef7e268d5b6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7c997ecbe6f48ea8e7f8f02ba915e3b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d1e3cc56efe94ff6a4688decc5b65514","IPY_MODEL_d9dcc2dc98c14e909b2c9049e75e901c","IPY_MODEL_2e95f7c2225c4461bda0394ca21781fa"],"layout":"IPY_MODEL_7efa4d24d9544bbd8924cb97dc3f09f6"}},"d1e3cc56efe94ff6a4688decc5b65514":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b943584824d48c28559928e7f7cf640","placeholder":"​","style":"IPY_MODEL_542bbf2711264a1ab751e9e053481f1c","value":"train-00000-of-00001.parquet: 100%"}},"d9dcc2dc98c14e909b2c9049e75e901c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b8ff60dfcde410f941bc71f75845a95","max":16369982,"min":0,"orientation":"horizontal","style":"IPY_MODEL_12320af89b9743a99d867a2a799907d7","value":16369982}},"2e95f7c2225c4461bda0394ca21781fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c621f7eafa54311b645ef8dc3e441aa","placeholder":"​","style":"IPY_MODEL_93a0f16e4d8e4daab85740dc930daadf","value":" 16.4M/16.4M [00:00&lt;00:00, 20.6MB/s]"}},"7efa4d24d9544bbd8924cb97dc3f09f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b943584824d48c28559928e7f7cf640":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"542bbf2711264a1ab751e9e053481f1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2b8ff60dfcde410f941bc71f75845a95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12320af89b9743a99d867a2a799907d7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c621f7eafa54311b645ef8dc3e441aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93a0f16e4d8e4daab85740dc930daadf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ef65d5557cf45deb01619935937be76":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a5a80b84260e47e5a4bab7816805b4ef","IPY_MODEL_7fd1d33b4d4a453eba9b4c49aa0bd253","IPY_MODEL_b1b9b258b39d4ec9ba22b0efdc128fec"],"layout":"IPY_MODEL_cf171d631cc54ec2913dad5beb66e22f"}},"a5a80b84260e47e5a4bab7816805b4ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc848b4535d34adcb93f56351ada3afc","placeholder":"​","style":"IPY_MODEL_bba5348cbedd4049a6f9c287e3885db9","value":"validation-00000-of-00001.parquet: 100%"}},"7fd1d33b4d4a453eba9b4c49aa0bd253":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f10735362314372a97ca710a139e162","max":1350511,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b2eecd32c6dc48b2979aa7cce44a949f","value":1350511}},"b1b9b258b39d4ec9ba22b0efdc128fec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f49ccf4e6fe144338828c187f0521a51","placeholder":"​","style":"IPY_MODEL_9f008c8c20e34b2ca649e5959cef0a6d","value":" 1.35M/1.35M [00:00&lt;00:00, 47.1MB/s]"}},"cf171d631cc54ec2913dad5beb66e22f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc848b4535d34adcb93f56351ada3afc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bba5348cbedd4049a6f9c287e3885db9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f10735362314372a97ca710a139e162":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2eecd32c6dc48b2979aa7cce44a949f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f49ccf4e6fe144338828c187f0521a51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f008c8c20e34b2ca649e5959cef0a6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed14d55dd7304d4cb62b591f2c2a42fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c86c4e148844bc4b4d32abaa401404f","IPY_MODEL_10084c71fe4640d5a5306cb51778c1a7","IPY_MODEL_c8eed43df7d74d70a0dd081c11c61a83"],"layout":"IPY_MODEL_ce2959d4b2ff4504bf2c3755ea8d95ab"}},"1c86c4e148844bc4b4d32abaa401404f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20805fd749ee44e9840c60a4e07f2862","placeholder":"​","style":"IPY_MODEL_58ccdfcb752e43478e3c318322a454db","value":"Generating train split: 100%"}},"10084c71fe4640d5a5306cb51778c1a7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16a2879ebb874f3eb6cb29c27636991a","max":130319,"min":0,"orientation":"horizontal","style":"IPY_MODEL_885b0ecd63294a09820f685aa5eb810b","value":130319}},"c8eed43df7d74d70a0dd081c11c61a83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37ae5f6df63f42ec91b15f57f31ec416","placeholder":"​","style":"IPY_MODEL_e746e2aa5229442d862d9caba1143041","value":" 130319/130319 [00:01&lt;00:00, 99851.90 examples/s]"}},"ce2959d4b2ff4504bf2c3755ea8d95ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20805fd749ee44e9840c60a4e07f2862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58ccdfcb752e43478e3c318322a454db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16a2879ebb874f3eb6cb29c27636991a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"885b0ecd63294a09820f685aa5eb810b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"37ae5f6df63f42ec91b15f57f31ec416":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e746e2aa5229442d862d9caba1143041":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f764fc2e5f44ac5a45b02774ea2ec29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c236b29dbefb453fb28172e5968b283c","IPY_MODEL_8520d7f506f64ed7a45100399f969700","IPY_MODEL_b90d52b32e5345c3852c6de783f59a91"],"layout":"IPY_MODEL_cbf350b5965d4a6d84ba48b197116dc7"}},"c236b29dbefb453fb28172e5968b283c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9194c64989214ca2b4e70d898b723d99","placeholder":"​","style":"IPY_MODEL_d3aa386bd05f4936953565d41cac7db5","value":"Generating validation split: 100%"}},"8520d7f506f64ed7a45100399f969700":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74f8aa87672443aeb57bfa79a8af9040","max":11873,"min":0,"orientation":"horizontal","style":"IPY_MODEL_219d0dccc5ac41f58341e28bcfdd432a","value":11873}},"b90d52b32e5345c3852c6de783f59a91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_889f67f2183e4062beab6687c915b54a","placeholder":"​","style":"IPY_MODEL_53a5b979512f4356a2813ee6dd2ca20f","value":" 11873/11873 [00:00&lt;00:00, 31614.90 examples/s]"}},"cbf350b5965d4a6d84ba48b197116dc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9194c64989214ca2b4e70d898b723d99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3aa386bd05f4936953565d41cac7db5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74f8aa87672443aeb57bfa79a8af9040":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"219d0dccc5ac41f58341e28bcfdd432a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"889f67f2183e4062beab6687c915b54a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53a5b979512f4356a2813ee6dd2ca20f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55d547027e8f43f2abb8770c6954e6ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b2aedc4d53b4622a1d0009f05c6c157","IPY_MODEL_08474d45bc4e495aa61dc1e00288e82f","IPY_MODEL_d3d6beee04724098b4cf24737ea775f3"],"layout":"IPY_MODEL_492b05b99a7a489d95069f2edbcc8d3b"}},"3b2aedc4d53b4622a1d0009f05c6c157":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6294197dd52413c8f986baba3878c25","placeholder":"​","style":"IPY_MODEL_e197d85b9e334d308bfd8d66d1d6f6a7","value":"Downloading builder script: 100%"}},"08474d45bc4e495aa61dc1e00288e82f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac9189434d34435d84557a1901a8454a","max":7021,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77eb7b72251d469389930fa9f9a2581b","value":7021}},"d3d6beee04724098b4cf24737ea775f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_103c8837dfc44d859eb05392b73b1005","placeholder":"​","style":"IPY_MODEL_bccf81c0dadf44b996b18ec4798fee4b","value":" 7.02k/7.02k [00:00&lt;00:00, 264kB/s]"}},"492b05b99a7a489d95069f2edbcc8d3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6294197dd52413c8f986baba3878c25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e197d85b9e334d308bfd8d66d1d6f6a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac9189434d34435d84557a1901a8454a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77eb7b72251d469389930fa9f9a2581b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"103c8837dfc44d859eb05392b73b1005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bccf81c0dadf44b996b18ec4798fee4b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":306,"referenced_widgets":["6037de7ef61e4406b19cf3bc2cdb9ee2","5070e42058cd47b2b0f9443b30b2a27c","5d230aae987f45bfa04b40834629d7f4","7d57b36a2ed246059cea60f278525a0c","7c0536b2cb3849e79d35a375a8d661d9","1cdaa9a7aecc457b900e44fc387fa848","0d9fa876669e4ffe88bbeb9cd6d80ada","45b79e8e3e854abea9e7efa4918ba9c8","e851b8ae16e54adab37dabe7d2d25dd4","2ce61eadf1094adcafecaa4afb0a78f8","eed1d436671b4c13abb0ef7e268d5b6b","f7c997ecbe6f48ea8e7f8f02ba915e3b","d1e3cc56efe94ff6a4688decc5b65514","d9dcc2dc98c14e909b2c9049e75e901c","2e95f7c2225c4461bda0394ca21781fa","7efa4d24d9544bbd8924cb97dc3f09f6","6b943584824d48c28559928e7f7cf640","542bbf2711264a1ab751e9e053481f1c","2b8ff60dfcde410f941bc71f75845a95","12320af89b9743a99d867a2a799907d7","4c621f7eafa54311b645ef8dc3e441aa","93a0f16e4d8e4daab85740dc930daadf","6ef65d5557cf45deb01619935937be76","a5a80b84260e47e5a4bab7816805b4ef","7fd1d33b4d4a453eba9b4c49aa0bd253","b1b9b258b39d4ec9ba22b0efdc128fec","cf171d631cc54ec2913dad5beb66e22f","fc848b4535d34adcb93f56351ada3afc","bba5348cbedd4049a6f9c287e3885db9","3f10735362314372a97ca710a139e162","b2eecd32c6dc48b2979aa7cce44a949f","f49ccf4e6fe144338828c187f0521a51","9f008c8c20e34b2ca649e5959cef0a6d","ed14d55dd7304d4cb62b591f2c2a42fb","1c86c4e148844bc4b4d32abaa401404f","10084c71fe4640d5a5306cb51778c1a7","c8eed43df7d74d70a0dd081c11c61a83","ce2959d4b2ff4504bf2c3755ea8d95ab","20805fd749ee44e9840c60a4e07f2862","58ccdfcb752e43478e3c318322a454db","16a2879ebb874f3eb6cb29c27636991a","885b0ecd63294a09820f685aa5eb810b","37ae5f6df63f42ec91b15f57f31ec416","e746e2aa5229442d862d9caba1143041","3f764fc2e5f44ac5a45b02774ea2ec29","c236b29dbefb453fb28172e5968b283c","8520d7f506f64ed7a45100399f969700","b90d52b32e5345c3852c6de783f59a91","cbf350b5965d4a6d84ba48b197116dc7","9194c64989214ca2b4e70d898b723d99","d3aa386bd05f4936953565d41cac7db5","74f8aa87672443aeb57bfa79a8af9040","219d0dccc5ac41f58341e28bcfdd432a","889f67f2183e4062beab6687c915b54a","53a5b979512f4356a2813ee6dd2ca20f"]},"id":"Vms2qgVRQeir","executionInfo":{"status":"ok","timestamp":1732035026827,"user_tz":-330,"elapsed":8329,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"2afef788-d515-457b-fb42-4e584bc00810"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6037de7ef61e4406b19cf3bc2cdb9ee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c997ecbe6f48ea8e7f8f02ba915e3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef65d5557cf45deb01619935937be76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed14d55dd7304d4cb62b591f2c2a42fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f764fc2e5f44ac5a45b02774ea2ec29"}},"metadata":{}}],"source":["# SQuad dataset\n","from datasets import load_dataset\n","\n","squad = load_dataset(\"rajpurkar/squad_v2\")"]},{"cell_type":"code","source":["# print(squad[\"validation\"])\n","validation_set = squad[\"validation\"]\n","import pandas as pd\n","\n","# Convert to pandas DataFrame\n","squad_test = validation_set.to_pandas()\n","\n","# Display first few rows\n","print(squad_test.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYWyr3rDVUp0","executionInfo":{"status":"ok","timestamp":1732036451413,"user_tz":-330,"elapsed":391,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"6f41d39e-294c-4c0b-ed3b-d5555f41a009"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["                         id    title  \\\n","0  56ddde6b9a695914005b9628  Normans   \n","1  56ddde6b9a695914005b9629  Normans   \n","2  56ddde6b9a695914005b962a  Normans   \n","3  56ddde6b9a695914005b962b  Normans   \n","4  56ddde6b9a695914005b962c  Normans   \n","\n","                                             context  \\\n","0  The Normans (Norman: Nourmands; French: Norman...   \n","1  The Normans (Norman: Nourmands; French: Norman...   \n","2  The Normans (Norman: Nourmands; French: Norman...   \n","3  The Normans (Norman: Nourmands; French: Norman...   \n","4  The Normans (Norman: Nourmands; French: Norman...   \n","\n","                                            question  \\\n","0               In what country is Normandy located?   \n","1                 When were the Normans in Normandy?   \n","2      From which countries did the Norse originate?   \n","3                          Who was the Norse leader?   \n","4  What century did the Normans first gain their ...   \n","\n","                                             answers  \n","0  {'text': ['France', 'France', 'France', 'Franc...  \n","1  {'text': ['10th and 11th centuries', 'in the 1...  \n","2  {'text': ['Denmark, Iceland and Norway', 'Denm...  \n","3  {'text': ['Rollo', 'Rollo', 'Rollo', 'Rollo'],...  \n","4  {'text': ['10th century', 'the first half of t...  \n"]}]},{"cell_type":"code","source":["squad_test.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-plcpyqs9Oc8","executionInfo":{"status":"ok","timestamp":1732045508582,"user_tz":-330,"elapsed":12,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"21a73e32-ae58-492b-96d0-a5e1c5416d32"},"execution_count":77,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['id', 'title', 'context', 'question', 'answers'], dtype='object')"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["# !pip install datasets"],"metadata":{"id":"sdhe9EnNU749","executionInfo":{"status":"ok","timestamp":1732035284220,"user_tz":-330,"elapsed":394,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","sst_path = kagglehub.dataset_download(\"atulanandjha/stanford-sentiment-treebank-v2-sst2\")\n","\n","print(\"Path to dataset files:\", sst)\n","# print(sst.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_h7pw-1nSaBm","executionInfo":{"status":"ok","timestamp":1732035348096,"user_tz":-330,"elapsed":3875,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"40f69bda-ac66-4e40-e9f6-f83ec98f1f7f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /root/.cache/kagglehub/datasets/atulanandjha/stanford-sentiment-treebank-v2-sst2/versions/30\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import io\n","\n","# Path to the CSV file (replace with actual filename in the dataset folder)\n","# uploaded = files.upload()\n","from google.colab import files\n","\n","# Upload the file\n","uploaded = files.upload()\n","\n","# Load the CSV into a DataFrame\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"id":"N7tYMIFIWd6w","executionInfo":{"status":"ok","timestamp":1732036294680,"user_tz":-330,"elapsed":11689,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"e7905d2d-a415-413d-f208-3f274ad1a032"},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-0e37ce24-95e0-43a0-a431-f5d486b139fa\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-0e37ce24-95e0-43a0-a431-f5d486b139fa\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving sst2_test.csv to sst2_test (1).csv\n"]}]},{"cell_type":"code","source":["sst2_test = pd.read_csv(\"sst2_test.csv\")\n","\n","# Display the first few rows\n","print(sst2_test.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNQl0EqPZ_cz","executionInfo":{"status":"ok","timestamp":1732036276705,"user_tz":-330,"elapsed":388,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"2f4bdd36-607f-424e-91e4-8e36612e3251"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["                                            sentence  label\n","0                     Effective but too-tepid biopic      2\n","1  If you sometimes like to go to the movies to h...      3\n","2  Emerges as something rare , an issue movie tha...      4\n","3  The film provides some great insight into the ...      2\n","4  Offers that rare combination of entertainment ...      4\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import io\n","\n","# Path to the CSV file (replace with actual filename in the dataset folder)\n","# uploaded = files.upload()\n","from google.colab import files\n","\n","# Upload the file\n","uploaded = files.upload()\n","\n","# Load the CSV into a DataFrame\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"DRMDj0B7aJzd","executionInfo":{"status":"ok","timestamp":1732037376885,"user_tz":-330,"elapsed":297463,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"9b519b28-1216-4cb3-f406-4930ea283090"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-38d9b37c-9246-4f3e-b1e6-f83e8a1ad6b6\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-38d9b37c-9246-4f3e-b1e6-f83e8a1ad6b6\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving stanford-sentiment-treebank-v2-sst2-metadata.json to stanford-sentiment-treebank-v2-sst2-metadata.json\n","Saving tokenizer.json to tokenizer.json\n","Saving tokenizer_config.json to tokenizer_config.json\n","Saving special_tokens_map.json to special_tokens_map.json\n","Saving config.json to config.json\n"]}]},{"cell_type":"code","source":["!pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9xQby-cQeRwq","executionInfo":{"status":"ok","timestamp":1732037402856,"user_tz":-330,"elapsed":8185,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"a37db580-0c9c-486e-b402-2135b283e0a4"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.1.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n","Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.6)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.26.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.17.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.3\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","# from datasets import load_metric\n","import evaluate\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# Set device to GPU if available\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"metadata":{"id":"6e-PaUhibtbM","executionInfo":{"status":"ok","timestamp":1732037420195,"user_tz":-330,"elapsed":363,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"de-lBzqCfK7z","executionInfo":{"status":"ok","timestamp":1732037895990,"user_tz":-330,"elapsed":28816,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"2092db8b-43a5-4637-8fad-f5849062acd0"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Q4"],"metadata":{"id":"0qWBQVa48fGe"}},{"cell_type":"code","source":["#Load the fine-tuned model\n","from safetensors.torch import load_file\n","from transformers import AutoModelForCausalLM, AutoConfig\n","\n","# Paths\n","weights_path = \"/content/drive/My Drive/model.safetensors\"\n","config_path = \"/content/drive/My Drive/config.json\"\n","\n","# Load configuration\n","config = AutoConfig.from_pretrained(config_path)\n","\n","# Load weights\n","state_dict = load_file(weights_path)\n","\n","# Load model with weights\n","model = AutoModelForCausalLM.from_config(config)\n","# model.load_state_dict(state_dict)\n","# model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"],"metadata":{"id":"ZYg6u41yfSXT","executionInfo":{"status":"ok","timestamp":1732045321760,"user_tz":-330,"elapsed":393,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":75,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"I1aXihURob_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(state_dict, strict=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OxvCmx69hFwe","executionInfo":{"status":"ok","timestamp":1732038141301,"user_tz":-330,"elapsed":14917,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"4252decf-2f24-4f52-a087-7f9dd7958b78"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=['lm_head.weight'], unexpected_keys=['score.weight'])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["import evaluate\n","\n","# Load metrics\n","classification_metrics = {\n","    \"accuracy\": evaluate.load(\"accuracy\"),\n","    \"precision\": evaluate.load(\"precision\"),\n","    \"recall\": evaluate.load(\"recall\"),\n","    \"f1\": evaluate.load(\"f1\"),\n","}\n","\n","qa_metrics = {\n","    \"squad_v2\": evaluate.load(\"squad_v2\"),\n","    \"bleu\": evaluate.load(\"bleu\"),\n","    \"rouge\": evaluate.load(\"rouge\"),\n","    \"meteor\": evaluate.load(\"meteor\"),\n","}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":122,"referenced_widgets":["55d547027e8f43f2abb8770c6954e6ee","3b2aedc4d53b4622a1d0009f05c6c157","08474d45bc4e495aa61dc1e00288e82f","d3d6beee04724098b4cf24737ea775f3","492b05b99a7a489d95069f2edbcc8d3b","d6294197dd52413c8f986baba3878c25","e197d85b9e334d308bfd8d66d1d6f6a7","ac9189434d34435d84557a1901a8454a","77eb7b72251d469389930fa9f9a2581b","103c8837dfc44d859eb05392b73b1005","bccf81c0dadf44b996b18ec4798fee4b"]},"id":"D8O0DjvndiDe","executionInfo":{"status":"ok","timestamp":1732038265464,"user_tz":-330,"elapsed":4272,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"5e5c8b57-5ed2-41c0-9418-88a74aaddd62"},"execution_count":43,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading builder script:   0%|          | 0.00/7.02k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d547027e8f43f2abb8770c6954e6ee"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]}]},{"cell_type":"code","source":["#Using fine-tuned  model"],"metadata":{"id":"SGsGhJO4sTgc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# classification\n","# sst2_test = pd.read_csv(\"sst2_test.csv\")\n","\n","# Create a classification pipeline\n","classification_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n","\n","# Predictions and evaluation\n","true_labels = sst2_test[\"label\"].tolist()\n","predicted_labels = []\n","for text in tqdm(sst2_test[\"sentence\"]):\n","    result = classification_pipeline(text)\n","    predicted_labels.append(int(result[0][\"label\"].split(\"_\")[-1]))  # Assuming labels are like \"LABEL_0\", \"LABEL_1\"\n","\n","# # Calculate metrics\n","# classification_results = {}\n","# for metric_name, metric in classification_metrics.items():\n","#     classification_results[metric_name] = metric.compute(predictions=predicted_labels, references=true_labels)\n","\n","# print(\"Classification Metrics:\", classification_results)\n"],"metadata":{"id":"_oJ9iDrhofCc","executionInfo":{"status":"ok","timestamp":1732044968293,"user_tz":-330,"elapsed":389,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["#Finding metrics from above, Classification part on fine-tuned model\n","from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n","import pandas as pd\n","\n","# Assuming true_labels and predicted_labels are your lists of true and predicted class labels\n","classification_results = {}\n","\n","# Get the classification report as a dictionary (instead of a string)\n","report = classification_report(true_labels, predicted_labels, output_dict=True)\n","\n","# Convert the classification report to a pandas DataFrame for a table-like display\n","report_df = pd.DataFrame(report).transpose()\n","\n","# Print the classification report in table format\n","print(\"Classification Report as Table:\")\n","print(report_df)\n","\n","# Alternatively, for individual metrics, you can compute and store them in a dictionary\n","f1 = f1_score(true_labels, predicted_labels, average='macro', zero_division=0)  # Use zero_division=0 or 1\n","classification_results['f1_score'] = f1\n","\n","precision = precision_score(true_labels, predicted_labels, average='macro', zero_division=0)  # or 1\n","classification_results['precision_score'] = precision\n","\n","recall = recall_score(true_labels, predicted_labels, average='macro', zero_division=0)  # or 1\n","classification_results['recall_score'] = recall\n","\n","# Optionally, print individual metrics in table format\n","classification_results_df = pd.DataFrame(classification_results, index=[0])\n","print(\"\\nIndividual Metrics as Table:\")\n","print(classification_results_df)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvrqP2iC5JVj","executionInfo":{"status":"ok","timestamp":1732044739187,"user_tz":-330,"elapsed":401,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"198f1656-93bb-4f96-d89d-e163788cea2c"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["Classification Report as Table:\n","              precision    recall  f1-score      support\n","0              0.127059  1.000000  0.225470   270.000000\n","1              0.000000  0.000000  0.000000   603.000000\n","2              0.000000  0.000000  0.000000   376.000000\n","3              0.000000  0.000000  0.000000   491.000000\n","4              0.000000  0.000000  0.000000   385.000000\n","accuracy       0.127059  0.127059  0.127059     0.127059\n","macro avg      0.025412  0.200000  0.045094  2125.000000\n","weighted avg   0.016144  0.127059  0.028648  2125.000000\n","\n","Individual Metrics as Table:\n","   f1_score  precision_score  recall_score\n","0  0.045094         0.025412           0.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"code","source":["def custom_qa_pipeline(question, context, max_length=256):\n","    prompt = f\"Question: {question}\\nContext: {context}\\nAnswer:\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n","\n","    # Remove `token_type_ids` if they exist\n","    inputs.pop(\"token_type_ids\", None)\n","\n","    # Generate the answer\n","    outputs = model.generate(\n","        **inputs, max_length=max_length, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id\n","    )\n","    answer = tokenizer.decode(outputs[0], skip_special_tokens=True).split(\"Answer:\")[-1].strip()\n","    return {\"answer\": answer}\n"],"metadata":{"id":"o5zPG7Uvomls","executionInfo":{"status":"ok","timestamp":1732046617953,"user_tz":-330,"elapsed":465,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","# Prepare input data\n","qa_input = [\n","    {\"question\": row[\"question\"], \"context\": row[\"context\"]}\n","    for _, row in squad_test.iterrows()\n","    if row[\"question\"] and row[\"context\"]  # Ensure non-empty inputs\n","]\n","\n","# Get predictions for the entire dataset\n","predictions = []\n","for qa in tqdm(qa_input, total=len(qa_input)):\n","    try:\n","        result = custom_qa_pipeline(qa[\"question\"], qa[\"context\"])\n","        predictions.append(result[\"answer\"])\n","    except Exception as e:\n","        predictions.append(\"Error\")  # Handle exceptions gracefully\n","        print(f\"Error processing question: {qa['question']}, Exception: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1quHJqRyBOla","executionInfo":{"status":"error","timestamp":1732048153064,"user_tz":-330,"elapsed":1532530,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"218be4ab-77ac-49bd-d25e-264d3d3001a3"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/11873 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 1/11873 [00:47<156:13:58, 47.38s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 2/11873 [01:33<153:28:21, 46.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 3/11873 [02:22<156:44:01, 47.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 4/11873 [03:09<156:23:52, 47.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 5/11873 [03:53<152:39:42, 46.31s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 6/11873 [04:34<146:48:14, 44.53s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 7/11873 [05:22<150:09:27, 45.56s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 8/11873 [06:07<149:46:44, 45.44s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 9/11873 [06:54<151:16:05, 45.90s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["Error processing question: Who was the duke in the battle of Hastings?, Exception: Input length of input_ids is 305, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n","Error processing question: Who ruled the duchy of Normandy, Exception: Input length of input_ids is 305, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n","Error processing question: What religion were the Normans, Exception: Input length of input_ids is 302, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n","Error processing question: What type of major impact did the Norman dynasty have on modern Europe?, Exception: Input length of input_ids is 309, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n","Error processing question: Who was famed for their Christian spirit?, Exception: Input length of input_ids is 303, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n","Error processing question: Who assimilted the Roman language?, Exception: Input length of input_ids is 303, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n","Error processing question: Who ruled the country of Normandy?, Exception: Input length of input_ids is 303, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n","Error processing question: What principality did William the conquerer found?, Exception: Input length of input_ids is 305, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 18/11873 [08:12<52:55:18, 16.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 19/11873 [09:29<75:26:15, 22.91s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 20/11873 [10:46<100:22:26, 30.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 21/11873 [12:06<127:03:18, 38.59s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 22/11873 [12:36<121:50:55, 37.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 23/11873 [13:01<113:31:53, 34.49s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 24/11873 [13:29<108:24:05, 32.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 25/11873 [13:55<102:27:55, 31.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 26/11873 [14:24<100:22:22, 30.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 27/11873 [14:52<98:11:16, 29.84s/it] Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 28/11873 [15:18<95:02:01, 28.88s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 29/11873 [16:09<115:24:11, 35.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 30/11873 [17:02<133:18:36, 40.52s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 31/11873 [17:56<146:23:44, 44.50s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 32/11873 [19:14<178:08:06, 54.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 33/11873 [20:27<196:42:40, 59.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 34/11873 [21:42<212:00:53, 64.47s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 35/11873 [22:57<221:47:14, 67.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 36/11873 [24:10<227:08:37, 69.08s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 37/11873 [25:11<219:29:11, 66.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","  0%|          | 37/11873 [25:30<135:57:37, 41.35s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-667e7fcb02fe>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mqa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_qa_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-90-b9f60daf2854>\u001b[0m in \u001b[0;36mcustom_qa_pipeline\u001b[0;34m(question, context, max_length)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Generate the answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2217\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1191\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    943\u001b[0m                 )\n\u001b[1;32m    944\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    946\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu\n","\n","# Example metric evaluation (adapt to your needs)\n","true_answers = [row[\"answers\"][\"text\"][0] for _, row in squad_test.iterrows()]\n","results = {\"rouge\": [], \"bleu\": []}\n","\n","for pred, true in zip(predictions, true_answers):\n","    rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True).score(true, pred)[\"rougeL\"].fmeasure\n","    bleu = sentence_bleu([true.split()], pred.split())\n","    results[\"rouge\"].append(rouge)\n","    results[\"bleu\"].append(bleu)\n","\n","# Print aggregated results\n","print(f\"ROUGE-L: {sum(results['rouge']) / len(results['rouge']):.2%}\")\n","print(f\"BLEU: {sum(results['bleu']) / len(results['bleu']):.2%}\")\n"],"metadata":{"id":"_rbo54oRBkZi","executionInfo":{"status":"aborted","timestamp":1732048153066,"user_tz":-330,"elapsed":20,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZJOb-yXp-8aW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"XA7PkWqMxLRi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load the pre-trained model"],"metadata":{"id":"hsA56PLBxLiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Using pre-trained zero-shot model\n","from safetensors.torch import load_file\n","from transformers import AutoModelForCausalLM, AutoConfig\n","\n","# Paths\n","weights_path = \"/content/drive/My Drive/pretrained_model.safetensors\"\n","config_path = \"/content/drive/My Drive/pretrained_cofig.json\"\n","\n","# Load configuration\n","config = AutoConfig.from_pretrained(config_path)\n","\n","# Load weights\n","state_dict = load_file(weights_path)\n","\n","# Load model with weights\n","pretrained_model = AutoModelForCausalLM.from_config(config)\n","# model.load_state_dict(state_dict)\n","# model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","pretrained_model.load_state_dict(state_dict, strict=False)\n","\n","\n","\n"],"metadata":{"id":"8TUEQY_Hs87H","executionInfo":{"status":"aborted","timestamp":1732044332685,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load pre-trained zero-shot model"],"metadata":{"id":"SlfgOs2BwLNT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# classification\n","\n","# Create a classification pipeline\n","classification_pipeline = pipeline(\"text-classification\", model=pretrained_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n","\n","# Predictions and evaluation\n","true_labels = sst2_test[\"label\"].tolist()\n","predicted_labels = []\n","for text in tqdm(sst2_test[\"sentence\"]):\n","    result = classification_pipeline(text)\n","    predicted_labels.append(int(result[0][\"label\"].split(\"_\")[-1]))  # Assuming labels are like \"LABEL_0\", \"LABEL_1\"\n","\n","# Calculate metrics\n","classification_results = {}\n","for metric_name, metric in classification_metrics.items():\n","    classification_results[metric_name] = metric.compute(predictions=predicted_labels, references=true_labels)\n","\n","print(\"Classification Metrics:\", classification_results)\n"],"metadata":{"id":"HPZUI9RZwGow","executionInfo":{"status":"aborted","timestamp":1732044332686,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# QA using fine-tuned model\n","\n","# Create a question-answering pipeline\n","qa_pipeline = pipeline(\"question-answering\", model=pretrained_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n","\n","# Predictions and evaluation\n","true_answers = []\n","predicted_answers = []\n","for i, row in tqdm(squad_test.iterrows(), total=len(squad_test)):\n","    result = qa_pipeline(question=row[\"question\"], context=row[\"context\"])\n","    predicted_answers.append(result[\"answer\"])\n","    true_answers.append({\"text\": row[\"answers\"], \"answer_start\": row[\"answer_start\"]})\n","\n","# Calculate metrics\n","qa_results = {}\n","qa_results[\"exact_match\"] = qa_metrics[\"squad_v2\"].compute(predictions=predicted_answers, references=true_answers)\n","qa_results[\"f1\"] = qa_metrics[\"f1\"].compute(predictions=predicted_answers, references=true_answers)\n","qa_results[\"meteor\"] = qa_metrics[\"meteor\"].compute(predictions=predicted_answers, references=true_answers)\n","qa_results[\"bleu\"] = qa_metrics[\"bleu\"].compute(predictions=predicted_answers, references=true_answers)\n","qa_results[\"rouge\"] = qa_metrics[\"rouge\"].compute(predictions=predicted_answers, references=true_answers)\n","\n","print(\"Question-Answering Metrics:\", qa_results)\n"],"metadata":{"id":"EMZQK7JcwWPA","executionInfo":{"status":"aborted","timestamp":1732044332689,"user_tz":-330,"elapsed":16,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"m2Iia3yUwaoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Q 5"],"metadata":{"id":"_kHtzdKD8UtO"}},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters())\n","\n","# Model parameters\n","\n","# Load fine-tuned model (if available)\n","fine_tuned_params = count_parameters(model)\n","\n","print(f\"Fine-Tuned Model Parameters: {fine_tuned_params}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zvIwjqHKouNl","executionInfo":{"status":"ok","timestamp":1732045071484,"user_tz":-330,"elapsed":408,"user":{"displayName":"Shivam Kumar 24210094","userId":"12963334657465969496"}},"outputId":"484d5fda-ebb6-4291-80f7-0154bde8f4b3"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["Fine-Tuned Model Parameters: 1235814400\n"]}]},{"cell_type":"code","source":["# Total parameters in pre-trained meta-llama/Llama-3.2-1B: 1235.82M (1235824640 parameters)\n","#So the number of parameters does not match in fine-tuned and pre-trained model"],"metadata":{"id":"YQM6FcaL76mt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Total parameters in pre-trained meta-llama/Llama-3.2-1B: 1235.82M (1235824640 parameters)\n","# Fine-Tuned Model Parameters: 1235814400\n","\n","#So the number of parameters does not match in fine-tuned and pre-trained model"],"metadata":{"id":"XE-a9B3j8FSO"}},{"cell_type":"code","source":[],"metadata":{"id":"yfMQVG1KG0Z4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Q7"],"metadata":{"id":"sbajPB2hG5wV"}},{"cell_type":"markdown","source":["1. Lower or Higher Scores in the Metrics [10 pts]\n","\n","\n","Higher Scores: Higher metric scores (like F1, BLEU, ROUGE, and Exact Match) indicate better alignment of the model's predictions with the ground truth. This reflects the model's ability to accurately understand and generate contextually relevant answers.\n","Lower Scores: Lower scores signify either a lack of understanding or issues in the model’s ability to generalize. This could be due to:\n","Insufficient fine-tuning on the task-specific data.\n","Lack of diverse data during fine-tuning or pretraining.\n","Poor handling of ambiguous or adversarial questions.\n","Rationale:\n","\n","F1 Score: Indicates precision and recall balance. A high score suggests that the predicted answers capture both relevance and completeness.\n","BLEU/ROUGE: These measure the overlap between predicted and reference answers. Low scores here might highlight issues with phrasing or inability to capture subtle nuances.\n","Exact Match: Useful for factual datasets like SQuAD, where the answer must exactly match. Lower EM might indicate paraphrasing or misunderstanding.\n","2. Understanding the Number of Parameters Between Pretraining and Fine-Tuning [05 pts]\n","\n","\n","Pretraining Parameters: Large models (e.g., LLaMA) have billions of parameters during pretraining, enabling them to learn general representations from diverse data. This is crucial for capturing broad language understanding.\n","Fine-Tuning Parameters: Fine-tuning often involves updating only a subset of parameters (e.g., LoRA or adapters) to adapt the general knowledge for specific tasks. In some cases, all parameters may be fine-tuned, but this is computationally expensive.\n","Rationale:\n","\n","The number of trainable parameters during fine-tuning directly affects adaptability. Fine-tuning a small subset is computationally efficient but might limit task-specific improvements.\n","Pretrained models with larger parameter counts usually generalize better but may need extensive fine-tuning to specialize.\n","3. Performance Difference for Zero-Shot and Fine-Tuned Models [05 pts]\n","\n","\n","Zero-Shot Performance: A pre-trained model is evaluated directly on a downstream task without any task-specific training. It relies on general language understanding and may perform well on tasks with close alignment to pretraining data.\n","Fine-Tuned Performance: Fine-tuned models are trained on specific datasets, improving their performance on the task. They learn task-specific nuances, which is critical for structured datasets like SQuAD."],"metadata":{"id":"Nx-gjY9FG-aH"}}]}