{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLZz7EqPHzWT"
   },
   "outputs": [],
   "source": [
    "# !pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGlLWfEGIHXB"
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THNaItYvHzWT"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers datasets torch scikit-learn\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "# !pip install safetensors\n",
    "# !pip install datasets\n",
    "# !pip install evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UhopmSUHzWU"
   },
   "source": [
    "### Get Test-Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcIQmv30HzWV",
    "outputId": "5a57b774-85b4-4185-f5f7-c65b7e5a18ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  label\n",
      "0  The Rock is destined to be the 21st Century 's...      3\n",
      "1  The gorgeously elaborate continuation of `` Th...      4\n",
      "2  Singer\\/composer Bryan Adams contributes a sle...      3\n",
      "3  You 'd think by now America would have had eno...      2\n",
      "4               Yet the act is still charming here .      3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(\"sst2_train.csv\")\n",
    "test_df = pd.read_csv(\"sst2_test.csv\")\n",
    "val_df = pd.read_csv(\"sst2_dev.csv\")\n",
    "\n",
    "# Display samples from the training dataset\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hTLxbD3HzWV",
    "outputId": "99366960-6003-4252-fdc7-5b3c3c07b08e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize datasets\n",
    "def tokenize_data(df):\n",
    "    return tokenizer(\n",
    "        df[\"sentence\"].tolist(),\n",
    "        # padding=True,\n",
    "        padding=\"max_length\",  # Pad to max sequence length\n",
    "        truncation=True,       # Truncate sequences longer than max length\n",
    "        max_length=128,        # Set max length to 128\n",
    "        return_tensors=\"pt\"    # Return PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Set the pad_token to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# # Add a custom pad token\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "\n",
    "train_encodings = tokenize_data(train_df)\n",
    "val_encodings = tokenize_data(val_df)\n",
    "test_encodings = tokenize_data(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGk0CR4JHzWW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SST2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = SST2Dataset(train_encodings, train_df[\"label\"].tolist())\n",
    "val_dataset = SST2Dataset(val_encodings, val_df[\"label\"].tolist())\n",
    "test_dataset = SST2Dataset(test_encodings, test_df[\"label\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9dR4s6LHzWW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZ-mmvzzHzWW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFEDm0jyHzWW"
   },
   "source": [
    "### Accessing Llama3.2-1B Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqJvIXcsHzWW",
    "outputId": "b85ba869-ccea-4ac1-b3be-29bff25b66c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=2048, out_features=5, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=5,  # Adjust for your classification task\n",
    "    torch_dtype=torch.float16,  # Mixed precision\n",
    "    device_map=\"auto\",           # Distributes the model across available GPUs\n",
    "    # device_map=\"cpu\"           # Distributes the model across available GPUs\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))  # Ensure the model handles new tokens\n",
    "\n",
    "# Add padding token if needed\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2lIUPbHHzWX"
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtBlXCptHzWX"
   },
   "outputs": [],
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj80LozdHzWX"
   },
   "outputs": [],
   "source": [
    "# # Prepare input text\n",
    "# input_text = \"The key to life is\"\n",
    "# inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(\"cuda\")  # Ensure tensors are on the correct device\n",
    "\n",
    "# # Generate outputs\n",
    "# outputs = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "# decoded_outputs = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# print(decoded_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkTxLXfRHzWX"
   },
   "source": [
    "### No. of Parameters~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6Byfz4THzWX",
    "outputId": "92fd3b5a-ccde-4bb6-c839-ca812921ff2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in meta-llama/Llama-3.2-1B: 1235.82M (1235824640 parameters)\n",
      "The calculated parameters DO NOT match the reported parameters in the paper.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Print results\n",
    "print(f\"Total parameters in {model_name}: {total_params / 1e6:.2f}M ({total_params} parameters)\")\n",
    "\n",
    "# Expected parameter count (from the paper)\n",
    "reported_params = 1e9  # 1 billion parameters\n",
    "\n",
    "# Compare the calculated count with the reported count\n",
    "if total_params == reported_params:\n",
    "    print(\"The calculated parameters match the reported parameters in the paper.\")\n",
    "else:\n",
    "    print(\"The calculated parameters DO NOT match the reported parameters in the paper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6nPJrwbHzWX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fW-czv4LHzWX"
   },
   "source": [
    "### Moddel Finetunning For Classification: SST-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xq9yFQXAHzWY"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxJdpHgaHzWY"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaAkDqpUHzWY"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Scheduler\n",
    "num_training_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lc_qlYGsKat5"
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "3oC_G7Y7gdAy",
    "outputId": "44fdcfd1-9ae4-49fe-ac35-401890135a13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjenil-patel910\u001b[0m (\u001b[33mjenil-patel910-indian-institute-of-technology-gandhinagar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20241119_131723-3nesjqpa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jenil-patel910-indian-institute-of-technology-gandhinagar/huggingface/runs/3nesjqpa' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/jenil-patel910-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jenil-patel910-indian-institute-of-technology-gandhinagar/huggingface' target=\"_blank\">https://wandb.ai/jenil-patel910-indian-institute-of-technology-gandhinagar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jenil-patel910-indian-institute-of-technology-gandhinagar/huggingface/runs/3nesjqpa' target=\"_blank\">https://wandb.ai/jenil-patel910-indian-institute-of-technology-gandhinagar/huggingface/runs/3nesjqpa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1524' max='1524' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1524/1524 27:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.733900</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1524, training_loss=0.5688745693897638, metrics={'train_runtime': 1631.1166, 'train_samples_per_second': 14.929, 'train_steps_per_second': 0.934, 'total_flos': 1.8199549691559936e+16, 'train_loss': 0.5688745693897638, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "uBBTX6zanh5e",
    "outputId": "850b8d11-c257-45cd-e009-85692a6f4639"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': nan, 'eval_runtime': 10.6763, 'eval_samples_per_second': 97.786, 'eval_steps_per_second': 6.182, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QG34QCahHzWY"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import evaluate\n",
    "\n",
    "# # Metric\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# # Training loop\n",
    "# epochs = 3\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# # device = torch.device(\"cpu\")\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "#     # Training phase\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "\n",
    "#     for batch in tqdm(train_dataloader):\n",
    "#         batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#         # print(\"in train, \")\n",
    "#         # Forward pass\n",
    "#         outputs = model(**batch)\n",
    "#         loss = outputs.loss\n",
    "#         train_loss += loss.item()\n",
    "\n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         lr_scheduler.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#     avg_train_loss = train_loss / len(train_dataloader)\n",
    "#     print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "#     # Evaluation phase\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch in val_dataloader:\n",
    "#             batch = {k: v.to(device) for k, v in batch.items()}\n",
    "#             # print(\"in validate, \")\n",
    "\n",
    "#             # Forward pass\n",
    "#             outputs = model(**batch)\n",
    "#             val_loss += outputs.loss.item()\n",
    "\n",
    "#             # Compute metrics\n",
    "#             logits = outputs.logits\n",
    "#             predictions = torch.argmax(logits, dim=-1)\n",
    "#             metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "#     avg_val_loss = val_loss / len(val_dataloader)\n",
    "#     val_accuracy = metric.compute()[\"accuracy\"]\n",
    "\n",
    "#     print(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nSgGebmHzWY",
    "outputId": "0bad3353-0520-4150-d90d-be8720ff697e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sst2_fine_tuned_tokenizer/tokenizer_config.json',\n",
       " './sst2_fine_tuned_tokenizer/special_tokens_map.json',\n",
       " './sst2_fine_tuned_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./sst2_fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./sst2_fine_tuned_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzl8Rr6lHzWY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ov9hAA6wHzWY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2p9SLFLHzWY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fs0I-8iPHzWY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_ZzlzzHHzWY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxYYHGdNHzWY",
    "outputId": "964b967b-f618-4660-cab8-ef297f0e4ed5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': nan}, {'label': 'LABEL_0', 'score': nan}]\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # Load the fine-tuned model\n",
    "# sentiment_pipeline = pipeline(\"text-classification\", model=\"./sst2_fine_tuned_model\", tokenizer=tokenizer)\n",
    "\n",
    "# # Test on new sentences\n",
    "# test_sentences = [\"I absolutely loved the movie!\", \"The food was horrible.\"]\n",
    "# predictions = sentiment_pipeline(test_sentences)\n",
    "\n",
    "# print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VouP3PmRshWS",
    "outputId": "9c6a52f7-de07-481b-9bb8-b76fae4db21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDXNPsTcHzWZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lTAk8k85HzWZ",
    "outputId": "abb8631c-4b33-4992-d48f-5f4fb9904dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: results/ (stored 0%)\n",
      "  adding: results/checkpoint-508/ (stored 0%)\n",
      "  adding: results/checkpoint-508/optimizer.pt (deflated 100%)\n",
      "  adding: results/checkpoint-508/training_args.bin (deflated 51%)\n",
      "  adding: results/checkpoint-508/model.safetensors (deflated 100%)\n",
      "  adding: results/checkpoint-508/trainer_state.json (deflated 56%)\n",
      "  adding: results/checkpoint-508/rng_state.pth (deflated 25%)\n",
      "  adding: results/checkpoint-508/scheduler.pt (deflated 55%)\n",
      "  adding: results/checkpoint-508/config.json (deflated 55%)\n",
      "  adding: results/checkpoint-1524/ (stored 0%)\n",
      "  adding: results/checkpoint-1524/optimizer.pt (deflated 100%)\n",
      "  adding: results/checkpoint-1524/training_args.bin (deflated 51%)\n",
      "  adding: results/checkpoint-1524/model.safetensors (deflated 100%)\n",
      "  adding: results/checkpoint-1524/trainer_state.json (deflated 66%)\n",
      "  adding: results/checkpoint-1524/rng_state.pth (deflated 25%)\n",
      "  adding: results/checkpoint-1524/scheduler.pt (deflated 56%)\n",
      "  adding: results/checkpoint-1524/config.json (deflated 55%)\n",
      "  adding: results/runs/ (stored 0%)\n",
      "  adding: results/runs/Nov19_13-17-19_b9dcf0cf1931/ (stored 0%)\n",
      "  adding: results/runs/Nov19_13-17-19_b9dcf0cf1931/events.out.tfevents.1732022240.b9dcf0cf1931.4617.0 (deflated 60%)\n",
      "  adding: results/runs/Nov19_13-17-19_b9dcf0cf1931/events.out.tfevents.1732023927.b9dcf0cf1931.4617.1 (deflated 25%)\n",
      "  adding: results/runs/Nov19_13-12-20_b9dcf0cf1931/ (stored 0%)\n",
      "  adding: results/runs/Nov19_13-12-20_b9dcf0cf1931/events.out.tfevents.1732021941.b9dcf0cf1931.275.2 (deflated 63%)\n",
      "  adding: results/runs/Nov19_13-07-31_b9dcf0cf1931/ (stored 0%)\n",
      "  adding: results/runs/Nov19_13-07-31_b9dcf0cf1931/events.out.tfevents.1732021655.b9dcf0cf1931.275.0 (deflated 62%)\n",
      "  adding: results/runs/Nov19_13-13-20_b9dcf0cf1931/ (stored 0%)\n",
      "  adding: results/runs/Nov19_13-13-20_b9dcf0cf1931/events.out.tfevents.1732022000.b9dcf0cf1931.275.3 (deflated 63%)\n",
      "  adding: results/runs/Nov19_13-14-42_b9dcf0cf1931/ (stored 0%)\n",
      "  adding: results/runs/Nov19_13-14-42_b9dcf0cf1931/events.out.tfevents.1732022084.b9dcf0cf1931.3833.0 (deflated 63%)\n",
      "  adding: results/runs/Nov19_13-10-44_b9dcf0cf1931/ (stored 0%)\n",
      "  adding: results/runs/Nov19_13-10-44_b9dcf0cf1931/events.out.tfevents.1732021846.b9dcf0cf1931.275.1 (deflated 62%)\n",
      "  adding: results/checkpoint-1016/ (stored 0%)\n",
      "  adding: results/checkpoint-1016/optimizer.pt\n",
      "\n",
      "\n",
      "zip error: Interrupted (aborting)\n"
     ]
    }
   ],
   "source": [
    "!zip -r data.zip results/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vZ0kmA7HzWZ",
    "outputId": "a410a35f-8791-4e99-af58-5d973620c9ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tzip warning: name not matched: /sst2_fine_tuned_model\n",
      "\tzip warning: name not matched: /sst2_fine_tuned_tokenizer\n",
      "  adding: sample_data/ (stored 0%)\n",
      "  adding: sample_data/anscombe.json (deflated 83%)\n",
      "  adding: sample_data/README.md (deflated 39%)\n",
      "  adding: sample_data/california_housing_train.csv (deflated 79%)\n",
      "  adding: sample_data/mnist_test.csv (deflated 88%)\n",
      "  adding: sample_data/mnist_train_small.csv\n",
      "\n",
      "\n",
      "zip error: Interrupted (aborting)\n"
     ]
    }
   ],
   "source": [
    "!zip -r data2.zip sample_data/ /sst2_fine_tuned_model /sst2_fine_tuned_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bljk938ktbjM",
    "outputId": "ac4535bd-863e-4090-f1e3-0c575c451dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/wandb/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/tmp/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/run-3nesjqpa.wandb (deflated 78%)\n",
      "  adding: content/wandb/latest-run/files/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/files/output.log (deflated 76%)\n",
      "  adding: content/wandb/latest-run/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/latest-run/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/latest-run/logs/ (stored 0%)\n",
      "  adding: content/wandb/latest-run/logs/debug-internal.log (deflated 80%)\n",
      "  adding: content/wandb/latest-run/logs/debug.log (deflated 75%)\n",
      "  adding: content/wandb/latest-run/logs/debug-core.log (deflated 58%)\n",
      "  adding: content/wandb/debug-internal.log (deflated 80%)\n",
      "  adding: content/wandb/debug.log (deflated 75%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/run-5ltc0fhj.wandb (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/files/wandb-summary.json (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/files/config.yaml (deflated 72%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/files/output.log (deflated 33%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/logs/debug-internal.log (deflated 74%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/logs/debug.log (deflated 68%)\n",
      "  adding: content/wandb/run-20241119_131446-5ltc0fhj/logs/debug-core.log (deflated 66%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/run-3nesjqpa.wandb (deflated 78%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/files/output.log (deflated 76%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/logs/debug-internal.log (deflated 80%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/logs/debug.log (deflated 75%)\n",
      "  adding: content/wandb/run-20241119_131723-3nesjqpa/logs/debug-core.log (deflated 58%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/tmp/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/tmp/code/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/run-elkihby4.wandb (deflated 83%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/files/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/files/wandb-summary.json (stored 0%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/files/config.yaml (deflated 72%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/files/output.log (deflated 63%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/files/requirements.txt (deflated 55%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/files/wandb-metadata.json (deflated 44%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/logs/ (stored 0%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/logs/debug-internal.log (deflated 79%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/logs/debug.log (deflated 88%)\n",
      "  adding: content/wandb/run-20241119_130839-elkihby4/logs/debug-core.log (deflated 69%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r wand.zip /content/wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WFQXHVZhtaXM",
    "outputId": "9c6a52f7-de07-481b-9bb8-b76fae4db21c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSUSKg0NHzWZ"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/sst2_fine_tuned_model/ /content/drive/MyDrive/NLP_A3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxfjWtt2uXXi"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/sst2_fine_tuned_tokenizer/ /content/drive/MyDrive/NLP_A3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XgUFiHLucCq"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/wandb/ /content/drive/MyDrive/NLP_A3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CZMliJlumGQ"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/results/ /content/drive/MyDrive/NLP_A3/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGWwuk9ruqDY"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
