{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers DataLoader huggingface_hub datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, DataCollatorWithPadding\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login(token=os.environ['HUGGINGFACE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# model.save_pretrained(\"./Model_QA\")\n",
    "# tokenizer.save_pretrained(\"./Tokenizer_QA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(current_dir+\"/Model_QA\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(current_dir+\"/Tokenizer_QA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rajpurkar/squad_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 5)\n",
      "(1000, 5)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"rajpurkar/squad_v2\")\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "\n",
    "\n",
    "    questions = examples[\"question\"]\n",
    "    contexts = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(tokenized_inputs[\"offset_mapping\"]):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0] if len(answer[\"answer_start\"]) > 0 else None\n",
    "        end_char = (\n",
    "            start_char + len(answer[\"text\"][0]) if start_char is not None else None\n",
    "        )\n",
    "\n",
    "        if start_char is None:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_pos, end_pos = None, None\n",
    "            for j, (start, end) in enumerate(offsets):\n",
    "                if start <= start_char < end:\n",
    "                    start_pos = j\n",
    "                if start < end_char <= end:\n",
    "                    end_pos = j\n",
    "            start_positions.append(start_pos)\n",
    "            end_positions.append(end_pos)\n",
    "\n",
    "    tokenized_inputs.pop(\"offset_mapping\")\n",
    "    tokenized_inputs[\"start_positions\"] = start_positions\n",
    "    tokenized_inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train_dataset = dataset[\"train\"].select(range(20000)).map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized_validation_dataset = dataset[\"validation\"].select(range(1000)).map(preprocess_function, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
    "\n",
    "print(tokenized_train_dataset.shape)\n",
    "print(tokenized_validation_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
    "\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_validation_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    tokenized_train_dataset, batch_size=128, shuffle=True, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    tokenized_validation_dataset, batch_size=32, shuffle=False, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    \n",
    "    if name not in [\"qa_outputs.weight\", \"qa_outputs.bias\"]:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [34:15<00:00, 13.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 1: 4.556686753679992\n",
      "Checkpoint saved at Model_checkpoints/checkpoint_epoch_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [34:31<00:00, 13.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 2: 4.323548414145305\n",
      "Checkpoint saved at Model_checkpoints/checkpoint_epoch_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [34:20<00:00, 13.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss for Epoch 3: 4.297109225753006\n",
      "Checkpoint saved at Model_checkpoints/checkpoint_epoch_3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_bert_qa/tokenizer_config.json',\n",
       " 'fine_tuned_bert_qa/special_tokens_map.json',\n",
       " 'fine_tuned_bert_qa/vocab.txt',\n",
       " 'fine_tuned_bert_qa/added_tokens.json',\n",
       " 'fine_tuned_bert_qa/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW([model.qa_outputs.weight, model.qa_outputs.bias],\n",
    "              lr=0.001)\n",
    "\n",
    "epochs = 3\n",
    "epoch_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"].to(device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(device),\n",
    "            start_positions=batch[\"start_positions\"].to(device).long(),\n",
    "            end_positions=batch[\"end_positions\"].to(device).long(),\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    epoch_loss.append(avg_loss)\n",
    "    print(f\"Average Loss for Epoch {epoch + 1}: {avg_loss}\")\n",
    "\n",
    "    checkpoint_dir = f\"Model_checkpoints/checkpoint_epoch_{epoch + 1}\"\n",
    "    model.save_pretrained(checkpoint_dir)\n",
    "    tokenizer.save_pretrained(checkpoint_dir)\n",
    "    print(f\"Checkpoint saved at {checkpoint_dir}\")\n",
    "\n",
    "\n",
    "model.save_pretrained(\"fine_tuned_bert_qa\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_bert_qa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate nltk rouge_score -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(\"/teamspace/studios/this_studio/Model_checkpoints/checkpoint_epoch_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /teamspace/studios/this_studio/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Exact Match': 0.0, 'BLEU Score': 0.0, 'ROUGE-L Score': 0.0, 'METEOR Score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from evaluate import load as load_metric\n",
    "import nltk\n",
    "from nltk.translate import meteor_score\n",
    "nltk.download('punkt')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Metrics\n",
    "f1_metric = load_metric(\"f1\")\n",
    "bleu_metric = load_metric(\"bleu\")\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "exact_match_metric = load_metric(\"exact_match\")\n",
    "\n",
    "# Helper functions\n",
    "def predict_answer(model, question, context):\n",
    "\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_scores[0])\n",
    "    end_idx = torch.argmax(end_scores[0])\n",
    "\n",
    "    if start_idx <= end_idx:\n",
    "\n",
    "        predicted_answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx + 1])\n",
    "    else:\n",
    "        predicted_answer = \"\"\n",
    "\n",
    "    return predicted_answer\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "\n",
    "    # Exact Match\n",
    "    exact_matches = [1 if pred.strip() == ref.strip() else 0 for pred, ref in zip(predictions, references)]\n",
    "    exact_match_score = sum(exact_matches) / len(exact_matches) * 100\n",
    "\n",
    "    # BLEU Score\n",
    "    bleu = bleu_metric.compute(predictions=predictions,\n",
    "                                references=references)['bleu'] * 100\n",
    "\n",
    "    # ROUGE Score\n",
    "    rouge = rouge_metric.compute(predictions=predictions, references=references, use_aggregator=True)\n",
    "    rouge_l = rouge['rougeL'] * 100\n",
    "\n",
    "    tokenized_predictions = [nltk.word_tokenize(pred) for pred in predictions]\n",
    "    tokenized_references = [nltk.word_tokenize(ref) for ref in references]\n",
    "\n",
    "    # Compute METEOR score\n",
    "    meteor = sum([meteor_score.single_meteor_score(ref, pred) for pred, ref in zip(tokenized_predictions, tokenized_references)]) / len(predictions) * 100\n",
    "\n",
    "    return {\n",
    "        \"Exact Match\": exact_match_score,\n",
    "        \"BLEU Score\": bleu,\n",
    "        \"ROUGE-L Score\": rouge_l,\n",
    "        \"METEOR Score\": meteor\n",
    "    }\n",
    "\n",
    "#Generate predictions and evaluate\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in dataset[\"validation\"].select(range(1000)):\n",
    "    question = sample['question']\n",
    "    context = sample['context']\n",
    "    reference = sample['answers']['text'][0] if sample['answers']['text'] else \"\"  # Use empty string for unanswerable\n",
    "\n",
    "    prediction = predict_answer(model, question, context)\n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_metrics(predictions, references)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_model = AutoModelForQuestionAnswering.from_pretrained(\"/teamspace/studios/this_studio/Model_QA\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Exact Match': 30.3, 'BLEU Score': 0.14726663803867085, 'ROUGE-L Score': 1.0665124093610086, 'METEOR Score': 2.0532087725175967}\n"
     ]
    }
   ],
   "source": [
    "#Generate predictions and evaluate\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for sample in dataset[\"validation\"].select(range(1000)):\n",
    "    question = sample['question']\n",
    "    context = sample['context']\n",
    "    reference = sample['answers']['text'][0] if sample['answers']['text'] else \"\"  # Use empty string for unanswerable\n",
    "\n",
    "    prediction = predict_answer(zero_shot_model, question, context)\n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "\n",
    "# Compute metrics\n",
    "metrics = compute_metrics(predictions, references)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuned Model Parameters: 1235818498\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Model parameters\n",
    "\n",
    "# Load fine-tuned model (if available)\n",
    "fine_tuned_params = count_parameters(model)\n",
    "\n",
    "print(f\"Fine-Tuned Model Parameters: {fine_tuned_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total parameters in pre-trained meta-llama/Llama-3.2-1B: 1235.82M (1235824640 parameters)\n",
    "Fine-Tuned Model Parameters: 1235814400\n",
    "So the number of parameters does not match in fine-tuned and pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7\n",
    "Lower or Higher Scores in the Metrics [10 pts]\n",
    "Higher Scores: Higher metric scores (like F1, BLEU, ROUGE, and Exact Match) indicate better alignment of the model's predictions with the ground truth. This reflects the model's ability to accurately understand and generate contextually relevant answers. Lower Scores: Lower scores signify either a lack of understanding or issues in the model’s ability to generalize. This could be due to: Insufficient fine-tuning on the task-specific data. Lack of diverse data during fine-tuning or pretraining. Poor handling of ambiguous or adversarial questions. Rationale:\n",
    "\n",
    "F1 Score: Indicates precision and recall balance. A high score suggests that the predicted answers capture both relevance and completeness. BLEU/ROUGE: These measure the overlap between predicted and reference answers. Low scores here might highlight issues with phrasing or inability to capture subtle nuances. Exact Match: Useful for factual datasets like SQuAD, where the answer must exactly match. Lower EM might indicate paraphrasing or misunderstanding. 2. Understanding the Number of Parameters Between Pretraining and Fine-Tuning [05 pts]\n",
    "\n",
    "Pretraining Parameters: Large models (e.g., LLaMA) have billions of parameters during pretraining, enabling them to learn general representations from diverse data. This is crucial for capturing broad language understanding. Fine-Tuning Parameters: Fine-tuning often involves updating only a subset of parameters (e.g., LoRA or adapters) to adapt the general knowledge for specific tasks. In some cases, all parameters may be fine-tuned, but this is computationally expensive. Rationale:\n",
    "\n",
    "The number of trainable parameters during fine-tuning directly affects adaptability. Fine-tuning a small subset is computationally efficient but might limit task-specific improvements. Pretrained models with larger parameter counts usually generalize better but may need extensive fine-tuning to specialize. 3. Performance Difference for Zero-Shot and Fine-Tuned Models [05 pts]\n",
    "\n",
    "Zero-Shot Performance: A pre-trained model is evaluated directly on a downstream task without any task-specific training. It relies on general language understanding and may perform well on tasks with close alignment to pretraining data. Fine-Tuned Performance: Fine-tuned models are trained on specific datasets, improving their performance on the task. They learn task-specific nuances, which is critical for structured datasets like SQuAD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
